{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Usage Cardinality Inference Demo\n",
    "**How much of a given dataset was used to train a machine learning model?** As AI continues to advance, this question becomes increasingly critical. According to Section 107 of the U.S. Copyright Act, determining whether a use constitutes fair use or copyright infringement requires evaluating the \"_amount and substantiality of the portion used in relation to the copyrighted work_\" under the \"_nature of the copyrighted work_.\"\n",
    "\n",
    "Dataset Usage Cardinality Inference (DUCI) enables data owners to estimate the exact proportion of dataset used, assessing the risk of unauthorized usage and protect their rights. DUCI achieves this through a debiasing process that aggregates individual Membership Inference Attack (MIA) guesses to deliver accurate results. More details are discussed in the [DUCI document](../documentation/duci.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "The Dataset Usage Cardinality Inference (DUCI) algorithm---acting as an agent for the dataset owner with full access to a target dataset---aims to estimate the proportion of the target dataset used in training a victim model, given black-box access to the model and knowledge of the training algorithm (e.g., the population data and model archtecture).\n",
    "\n",
    "<img src=\"documentation/images/duci_problem.png\" alt=\"Problem Illustration\" title=\"Simple DUCI Pipeline\" width=\"600\">\n",
    "\n",
    "## Method\n",
    "\n",
    "To estimate the proportion of a target dataset being used, the Dataset Usage Cardinality Inference (DUCI) algorithm first debiases the membership predictions \\(\\hat{m}_i\\) provided by any Membership Inference Attack (MIA) method to obtain the probability of each data record being used, using the following formula:\n",
    "\n",
    "$\\hat{p}_i = \\frac{\\hat{m}_i - P(\\hat{m}_i = 1 \\mid m_i = 0)}{P(\\hat{m}_i = 1 \\mid m_i = 1) - P(\\hat{m}_i = 1 \\mid m_i = 0)}$,\n",
    "\n",
    "After debiasing, DUCI aggregates the unbiased probability estimators over the entire dataset to compute the overall proportion:\n",
    "\n",
    "$\\hat{p} = \\frac{1}{|X|} \\sum_{i=1}^{|X|} \\hat{p}_i,$\n",
    "\n",
    "where $|X|$ is the size of the target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone the github repo\n",
    "# !git clone https://github.com/privacytrustlab/ml_privacy_meter.git\n",
    "\n",
    "# # Update the Colab environment\n",
    "# !pip install datasets==2.21.0 transformers==4.44.2 torch==2.4.1 torchvision==0.19.1 torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change the directory to the cloned repo\n",
    "# import sys\n",
    "# sys.path.append('/content/ml_privacy_meter')\n",
    "\n",
    "# %cd ml_privacy_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy torch  # Install NumPy and PyTorch if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-12 15:42:35,531 - INFO - PyTorch version 2.4.0 available.\n",
      "2025-02-12 15:42:36,825 - DEBUG - matplotlib data path: /home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/matplotlib/mpl-data\n",
      "2025-02-12 15:42:36,829 - DEBUG - CONFIGDIR=/home/yao/.config/matplotlib\n",
      "2025-02-12 15:42:36,830 - DEBUG - interactive is False\n",
      "2025-02-12 15:42:36,831 - DEBUG - platform is linux\n",
      "2025-02-12 15:42:37,021 - DEBUG - CACHEDIR=/home/yao/.cache/matplotlib\n",
      "2025-02-12 15:42:37,024 - DEBUG - Using fontManager instance from /home/yao/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "from privacy_meter.dataset import get_dataset\n",
    "from privacy_meter.models.utils import train_models, load_models\n",
    "from privacy_meter.get_signals import get_model_signals\n",
    "from privacy_meter.modules.mia import MIA\n",
    "from privacy_meter.modules.duci import DUCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "As the dataset owner, we have a target dataset $X$ and access to a population pool. For simplicity, assume the population pool is the CIFAR-10 dataset, and we sample a subset $X$ of size 500 from this pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Configs\n",
    "_dataset = 'cifar10' # cifar10 as the population pool\n",
    "dataset_dir = '../data'\n",
    "log_dir = 'demo_duci'\n",
    "configs = {\n",
    "    'run': {\n",
    "        'random_seed': 12345,\n",
    "        'log_dir': 'demo_duci',\n",
    "        'time_log': True,\n",
    "        'num_experiments': 1\n",
    "    },\n",
    "    'audit': {\n",
    "        'privacy_game': 'privacy_loss_model',\n",
    "        'algorithm': 'RMIA',\n",
    "        'num_ref_models': 1,\n",
    "        'device': 'cuda:0',\n",
    "        'report_log': 'report_rmia',\n",
    "        'batch_size': 5000\n",
    "    },\n",
    "    'train': {\n",
    "        'model_name': 'wrn28-2',\n",
    "        'device': 'cuda:0',\n",
    "        'batch_size': 256,\n",
    "        'optimizer': 'SGD',\n",
    "        'learning_rate': 0.1,\n",
    "        'weight_decay': 0,\n",
    "        'epochs': 100\n",
    "    },\n",
    "    'data': {\n",
    "        'dataset': 'cifar10',\n",
    "        'data_dir': 'data'\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:42:37,267 - INFO - Data loaded from data/cifar10.pkl\n",
      "2025-02-12 15:42:37,296 - INFO - Population data loaded from data/cifar10_population.pkl\n",
      "2025-02-12 15:42:37,297 - INFO - The whole dataset size: 50000\n"
     ]
    }
   ],
   "source": [
    "dataset, population = get_dataset(_dataset, dataset_dir, logger)\n",
    "\n",
    "# Select 500 points from the dataset as the target dataset\n",
    "all_indices = list(range(len(dataset)))\n",
    "random.shuffle(all_indices)\n",
    "target_indices = all_indices[:500]\n",
    "remaining_indices = all_indices[500:]\n",
    "TRAIN_SIZE = len(dataset) // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the victim model\n",
    "Suppose a victim model is trained on a randomly selected $p$ proportion of our dataset $X$. Our goal is to infer the value of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "p = random.choice(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selection 0.3 proportion of the target dataset\n",
    "selected_indices = random.sample(target_indices, int(p * len(target_indices)))\n",
    "remaining_size = TRAIN_SIZE - len(selected_indices)\n",
    "selected_remaining_indices = random.sample(remaining_indices, remaining_size)\n",
    "selected_victim_indices = selected_indices + selected_remaining_indices\n",
    "\n",
    "# select all unselected indices in all_indices as the test indices\n",
    "test_indices = list(set(all_indices) - set(selected_indices) - set(selected_remaining_indices))\n",
    "target_data_split = {\n",
    "    'train': selected_victim_indices,\n",
    "    'test': test_indices\n",
    "}\n",
    "target_membership = np.zeros(len(dataset))\n",
    "target_membership[selected_victim_indices] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train reference models\n",
    "\n",
    "In the **Privacy Meter** library, $2N$ reference models are trained by default, ensuring that each data point is included in one model's training set and excluded from another. We first explore dataset usage inference using two reference models before moving to the special case of single-reference models (by adapting the MIA implementations in the library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selection half of the target dataset\n",
    "ref_selected_indices = random.sample(target_indices, int(0.5 * len(target_indices)))\n",
    "ref_remaining_size = TRAIN_SIZE - len(ref_selected_indices)\n",
    "ref_selected_remaining_indices = random.sample(remaining_indices, ref_remaining_size)\n",
    "ref_selected_victim_indices = ref_selected_indices + ref_selected_remaining_indices\n",
    "\n",
    "# select all unselected indices in all_indices as the test indices\n",
    "ref_test_indices = list(set(all_indices) - set(ref_selected_indices) - set(ref_selected_remaining_indices))\n",
    "ref_data_split = {\n",
    "    'train': ref_selected_victim_indices,\n",
    "    'test': ref_test_indices\n",
    "}\n",
    "ref_membership = np.zeros(len(dataset))\n",
    "ref_membership[ref_selected_victim_indices] = 1\n",
    "\n",
    "# Get the pair reference model\n",
    "# ref_paired_selected_indices = list(set(target_indices)-set(ref_selected_indices))\n",
    "# ref_paired_remaining_size = TRAIN_SIZE - len(ref_paired_selected_indices)\n",
    "# ref_paired_selected_remaining_indices = random.sample(remaining_indices, ref_paired_remaining_size)\n",
    "# ref_paired_selected_victim_indices = ref_paired_selected_indices + ref_paired_selected_remaining_indices\n",
    "ref_paired_selected_indices = random.sample(target_indices, int(0.5 * len(target_indices)))\n",
    "ref_paired_remaining_size = TRAIN_SIZE - len(ref_paired_selected_indices)\n",
    "ref_paired_selected_remaining_indices = random.sample(remaining_indices, ref_paired_remaining_size)\n",
    "ref_paired_selected_victim_indices = ref_paired_selected_indices + ref_paired_selected_remaining_indices\n",
    "\n",
    "# select all unselected indices in all_indices as the test indices\n",
    "ref_paired_test_indices = list(set(all_indices) - set(ref_paired_selected_indices) - set(ref_paired_selected_remaining_indices))\n",
    "ref_paired_data_split = {\n",
    "    'train': ref_paired_selected_victim_indices,\n",
    "    'test': ref_paired_test_indices\n",
    "}\n",
    "ref_paired_membership = np.zeros(len(dataset))\n",
    "ref_paired_membership[ref_paired_selected_victim_indices] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:42:37,449 - INFO - Training 3 models\n",
      "2025-02-12 15:42:37,452 - INFO - --------------------------------------------------\n",
      "2025-02-12 15:42:37,453 - INFO - Training model 0: Train size 25000, Test size 25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimizer: SGD | Learning Rate: 0.1 | Weight Decay: 0\n",
      "Epoch [1/100] | Train Loss: 2.2693 | Train Acc: 0.1566\n",
      "Test Loss: 2.1526 | Test Acc: 0.2058\n",
      "Epoch 1 took 5.23 seconds\n",
      "Epoch [2/100] | Train Loss: 1.9806 | Train Acc: 0.2658\n",
      "Test Loss: 1.8610 | Test Acc: 0.2988\n",
      "Epoch 2 took 4.51 seconds\n",
      "Epoch [3/100] | Train Loss: 1.7937 | Train Acc: 0.3288\n",
      "Test Loss: 1.7606 | Test Acc: 0.3361\n",
      "Epoch 3 took 4.50 seconds\n",
      "Epoch [4/100] | Train Loss: 1.6903 | Train Acc: 0.3722\n",
      "Test Loss: 1.6698 | Test Acc: 0.3763\n",
      "Epoch 4 took 4.48 seconds\n",
      "Epoch [5/100] | Train Loss: 1.6038 | Train Acc: 0.4046\n",
      "Test Loss: 1.6139 | Test Acc: 0.3992\n",
      "Epoch 5 took 4.50 seconds\n",
      "Epoch [6/100] | Train Loss: 1.5172 | Train Acc: 0.4366\n",
      "Test Loss: 1.5782 | Test Acc: 0.4183\n",
      "Epoch 6 took 4.52 seconds\n",
      "Epoch [7/100] | Train Loss: 1.4409 | Train Acc: 0.4716\n",
      "Test Loss: 1.5245 | Test Acc: 0.4196\n",
      "Epoch 7 took 4.48 seconds\n",
      "Epoch [8/100] | Train Loss: 1.3672 | Train Acc: 0.5030\n",
      "Test Loss: 1.4208 | Test Acc: 0.4804\n",
      "Epoch 8 took 4.48 seconds\n",
      "Epoch [9/100] | Train Loss: 1.3008 | Train Acc: 0.5289\n",
      "Test Loss: 1.3930 | Test Acc: 0.5001\n",
      "Epoch 9 took 4.51 seconds\n",
      "Epoch [10/100] | Train Loss: 1.2308 | Train Acc: 0.5584\n",
      "Test Loss: 1.3502 | Test Acc: 0.5087\n",
      "Epoch 10 took 4.50 seconds\n",
      "Epoch [11/100] | Train Loss: 1.1735 | Train Acc: 0.5803\n",
      "Test Loss: 2.1193 | Test Acc: 0.3442\n",
      "Epoch 11 took 4.53 seconds\n",
      "Epoch [12/100] | Train Loss: 1.1215 | Train Acc: 0.5986\n",
      "Test Loss: 1.2797 | Test Acc: 0.5379\n",
      "Epoch 12 took 4.50 seconds\n",
      "Epoch [13/100] | Train Loss: 1.0676 | Train Acc: 0.6198\n",
      "Test Loss: 1.4570 | Test Acc: 0.4799\n",
      "Epoch 13 took 4.52 seconds\n",
      "Epoch [14/100] | Train Loss: 1.0217 | Train Acc: 0.6364\n",
      "Test Loss: 1.2802 | Test Acc: 0.5455\n",
      "Epoch 14 took 4.51 seconds\n",
      "Epoch [15/100] | Train Loss: 0.9680 | Train Acc: 0.6564\n",
      "Test Loss: 2.3478 | Test Acc: 0.3900\n",
      "Epoch 15 took 4.49 seconds\n",
      "Epoch [16/100] | Train Loss: 0.9234 | Train Acc: 0.6761\n",
      "Test Loss: 1.4481 | Test Acc: 0.5071\n",
      "Epoch 16 took 4.52 seconds\n",
      "Epoch [17/100] | Train Loss: 0.8763 | Train Acc: 0.6914\n",
      "Test Loss: 1.5726 | Test Acc: 0.4697\n",
      "Epoch 17 took 4.52 seconds\n",
      "Epoch [18/100] | Train Loss: 0.8289 | Train Acc: 0.7107\n",
      "Test Loss: 1.4713 | Test Acc: 0.5062\n",
      "Epoch 18 took 4.55 seconds\n",
      "Epoch [19/100] | Train Loss: 0.7806 | Train Acc: 0.7282\n",
      "Test Loss: 1.9684 | Test Acc: 0.4505\n",
      "Epoch 19 took 4.52 seconds\n",
      "Epoch [20/100] | Train Loss: 0.7329 | Train Acc: 0.7497\n",
      "Test Loss: 1.5438 | Test Acc: 0.5170\n",
      "Epoch 20 took 4.54 seconds\n",
      "Epoch [21/100] | Train Loss: 0.6978 | Train Acc: 0.7588\n",
      "Test Loss: 1.3788 | Test Acc: 0.5324\n",
      "Epoch 21 took 4.53 seconds\n",
      "Epoch [22/100] | Train Loss: 0.6378 | Train Acc: 0.7838\n",
      "Test Loss: 1.5578 | Test Acc: 0.5284\n",
      "Epoch 22 took 4.51 seconds\n",
      "Epoch [23/100] | Train Loss: 0.6031 | Train Acc: 0.7948\n",
      "Test Loss: 3.8066 | Test Acc: 0.3886\n",
      "Epoch 23 took 4.52 seconds\n",
      "Epoch [24/100] | Train Loss: 0.5407 | Train Acc: 0.8189\n",
      "Test Loss: 1.2847 | Test Acc: 0.5692\n",
      "Epoch 24 took 4.51 seconds\n",
      "Epoch [25/100] | Train Loss: 0.4965 | Train Acc: 0.8368\n",
      "Test Loss: 1.7371 | Test Acc: 0.5206\n",
      "Epoch 25 took 4.50 seconds\n",
      "Epoch [26/100] | Train Loss: 0.4580 | Train Acc: 0.8532\n",
      "Test Loss: 1.8366 | Test Acc: 0.5020\n",
      "Epoch 26 took 4.52 seconds\n",
      "Epoch [27/100] | Train Loss: 0.4176 | Train Acc: 0.8702\n",
      "Test Loss: 1.7402 | Test Acc: 0.5098\n",
      "Epoch 27 took 4.53 seconds\n",
      "Epoch [28/100] | Train Loss: 0.3559 | Train Acc: 0.8904\n",
      "Test Loss: 1.3751 | Test Acc: 0.5857\n",
      "Epoch 28 took 4.52 seconds\n",
      "Epoch [29/100] | Train Loss: 0.3116 | Train Acc: 0.9100\n",
      "Test Loss: 1.8690 | Test Acc: 0.4914\n",
      "Epoch 29 took 4.53 seconds\n",
      "Epoch [30/100] | Train Loss: 0.2638 | Train Acc: 0.9287\n",
      "Test Loss: 1.4907 | Test Acc: 0.5574\n",
      "Epoch 30 took 4.55 seconds\n",
      "Epoch [31/100] | Train Loss: 0.2755 | Train Acc: 0.9226\n",
      "Test Loss: 1.6428 | Test Acc: 0.5539\n",
      "Epoch 31 took 4.53 seconds\n",
      "Epoch [32/100] | Train Loss: 0.1832 | Train Acc: 0.9586\n",
      "Test Loss: 1.4667 | Test Acc: 0.5821\n",
      "Epoch 32 took 4.53 seconds\n",
      "Epoch [33/100] | Train Loss: 0.1375 | Train Acc: 0.9736\n",
      "Test Loss: 1.4140 | Test Acc: 0.5926\n",
      "Epoch 33 took 4.54 seconds\n",
      "Epoch [34/100] | Train Loss: 0.1027 | Train Acc: 0.9852\n",
      "Test Loss: 1.6116 | Test Acc: 0.5582\n",
      "Epoch 34 took 4.53 seconds\n",
      "Epoch [35/100] | Train Loss: 0.0708 | Train Acc: 0.9930\n",
      "Test Loss: 1.4733 | Test Acc: 0.5877\n",
      "Epoch 35 took 4.52 seconds\n",
      "Epoch [36/100] | Train Loss: 0.0532 | Train Acc: 0.9964\n",
      "Test Loss: 1.5744 | Test Acc: 0.5723\n",
      "Epoch 36 took 4.52 seconds\n",
      "Epoch [37/100] | Train Loss: 0.0411 | Train Acc: 0.9983\n",
      "Test Loss: 1.5781 | Test Acc: 0.5692\n",
      "Epoch 37 took 4.57 seconds\n",
      "Epoch [38/100] | Train Loss: 0.0336 | Train Acc: 0.9990\n",
      "Test Loss: 1.3727 | Test Acc: 0.6138\n",
      "Epoch 38 took 4.54 seconds\n",
      "Epoch [39/100] | Train Loss: 0.0272 | Train Acc: 0.9993\n",
      "Test Loss: 1.2985 | Test Acc: 0.6222\n",
      "Epoch 39 took 4.52 seconds\n",
      "Epoch [40/100] | Train Loss: 0.0243 | Train Acc: 0.9996\n",
      "Test Loss: 1.4525 | Test Acc: 0.5994\n",
      "Epoch 40 took 4.51 seconds\n",
      "Epoch [41/100] | Train Loss: 0.0207 | Train Acc: 0.9996\n",
      "Test Loss: 1.4303 | Test Acc: 0.6106\n",
      "Epoch 41 took 4.54 seconds\n",
      "Epoch [42/100] | Train Loss: 0.0189 | Train Acc: 0.9996\n",
      "Test Loss: 1.3773 | Test Acc: 0.6166\n",
      "Epoch 42 took 4.52 seconds\n",
      "Epoch [43/100] | Train Loss: 0.0164 | Train Acc: 0.9997\n",
      "Test Loss: 1.4177 | Test Acc: 0.6111\n",
      "Epoch 43 took 4.51 seconds\n",
      "Epoch [44/100] | Train Loss: 0.0153 | Train Acc: 0.9998\n",
      "Test Loss: 1.3664 | Test Acc: 0.6206\n",
      "Epoch 44 took 4.56 seconds\n",
      "Epoch [45/100] | Train Loss: 0.0138 | Train Acc: 0.9998\n",
      "Test Loss: 1.4642 | Test Acc: 0.6072\n",
      "Epoch 45 took 4.52 seconds\n",
      "Epoch [46/100] | Train Loss: 0.0129 | Train Acc: 0.9998\n",
      "Test Loss: 1.3850 | Test Acc: 0.6200\n",
      "Epoch 46 took 4.50 seconds\n",
      "Epoch [47/100] | Train Loss: 0.0125 | Train Acc: 0.9998\n",
      "Test Loss: 1.3948 | Test Acc: 0.6150\n",
      "Epoch 47 took 4.51 seconds\n",
      "Epoch [48/100] | Train Loss: 0.0115 | Train Acc: 0.9999\n",
      "Test Loss: 1.4238 | Test Acc: 0.6166\n",
      "Epoch 48 took 4.51 seconds\n",
      "Epoch [49/100] | Train Loss: 0.0107 | Train Acc: 0.9999\n",
      "Test Loss: 1.4102 | Test Acc: 0.6147\n",
      "Epoch 49 took 4.55 seconds\n",
      "Epoch [50/100] | Train Loss: 0.0103 | Train Acc: 0.9999\n",
      "Test Loss: 1.4192 | Test Acc: 0.6153\n",
      "Epoch 50 took 4.56 seconds\n",
      "Epoch [51/100] | Train Loss: 0.0097 | Train Acc: 1.0000\n",
      "Test Loss: 1.4427 | Test Acc: 0.6158\n",
      "Epoch 51 took 4.53 seconds\n",
      "Epoch [52/100] | Train Loss: 0.0092 | Train Acc: 1.0000\n",
      "Test Loss: 1.4296 | Test Acc: 0.6148\n",
      "Epoch 52 took 4.53 seconds\n",
      "Epoch [53/100] | Train Loss: 0.0089 | Train Acc: 1.0000\n",
      "Test Loss: 1.4279 | Test Acc: 0.6174\n",
      "Epoch 53 took 4.54 seconds\n",
      "Epoch [54/100] | Train Loss: 0.0084 | Train Acc: 1.0000\n",
      "Test Loss: 1.4354 | Test Acc: 0.6144\n",
      "Epoch 54 took 4.53 seconds\n",
      "Epoch [55/100] | Train Loss: 0.0083 | Train Acc: 1.0000\n",
      "Test Loss: 1.4258 | Test Acc: 0.6143\n",
      "Epoch 55 took 4.53 seconds\n",
      "Epoch [56/100] | Train Loss: 0.0075 | Train Acc: 1.0000\n",
      "Test Loss: 1.4819 | Test Acc: 0.6107\n",
      "Epoch 56 took 4.56 seconds\n",
      "Epoch [57/100] | Train Loss: 0.0073 | Train Acc: 1.0000\n",
      "Test Loss: 1.4642 | Test Acc: 0.6136\n",
      "Epoch 57 took 4.53 seconds\n",
      "Epoch [58/100] | Train Loss: 0.0071 | Train Acc: 1.0000\n",
      "Test Loss: 1.4883 | Test Acc: 0.6119\n",
      "Epoch 58 took 4.51 seconds\n",
      "Epoch [59/100] | Train Loss: 0.0067 | Train Acc: 1.0000\n",
      "Test Loss: 1.5183 | Test Acc: 0.6077\n",
      "Epoch 59 took 4.51 seconds\n",
      "Epoch [60/100] | Train Loss: 0.0067 | Train Acc: 1.0000\n",
      "Test Loss: 1.4513 | Test Acc: 0.6171\n",
      "Epoch 60 took 4.53 seconds\n",
      "Epoch [61/100] | Train Loss: 0.0067 | Train Acc: 1.0000\n",
      "Test Loss: 1.4969 | Test Acc: 0.6110\n",
      "Epoch 61 took 4.52 seconds\n",
      "Epoch [62/100] | Train Loss: 0.0064 | Train Acc: 1.0000\n",
      "Test Loss: 1.4519 | Test Acc: 0.6158\n",
      "Epoch 62 took 4.52 seconds\n",
      "Epoch [63/100] | Train Loss: 0.0064 | Train Acc: 1.0000\n",
      "Test Loss: 1.5156 | Test Acc: 0.6114\n",
      "Epoch 63 took 4.49 seconds\n",
      "Epoch [64/100] | Train Loss: 0.0060 | Train Acc: 1.0000\n",
      "Test Loss: 1.4704 | Test Acc: 0.6144\n",
      "Epoch 64 took 4.54 seconds\n",
      "Epoch [65/100] | Train Loss: 0.0057 | Train Acc: 1.0000\n",
      "Test Loss: 1.4739 | Test Acc: 0.6126\n",
      "Epoch 65 took 4.52 seconds\n",
      "Epoch [66/100] | Train Loss: 0.0056 | Train Acc: 1.0000\n",
      "Test Loss: 1.4706 | Test Acc: 0.6169\n",
      "Epoch 66 took 4.53 seconds\n",
      "Epoch [67/100] | Train Loss: 0.0054 | Train Acc: 1.0000\n",
      "Test Loss: 1.4764 | Test Acc: 0.6127\n",
      "Epoch 67 took 4.52 seconds\n",
      "Epoch [68/100] | Train Loss: 0.0053 | Train Acc: 1.0000\n",
      "Test Loss: 1.4683 | Test Acc: 0.6140\n",
      "Epoch 68 took 4.56 seconds\n",
      "Epoch [69/100] | Train Loss: 0.0050 | Train Acc: 1.0000\n",
      "Test Loss: 1.4669 | Test Acc: 0.6184\n",
      "Epoch 69 took 4.51 seconds\n",
      "Epoch [70/100] | Train Loss: 0.0051 | Train Acc: 1.0000\n",
      "Test Loss: 1.5130 | Test Acc: 0.6118\n",
      "Epoch 70 took 4.52 seconds\n",
      "Epoch [71/100] | Train Loss: 0.0048 | Train Acc: 1.0000\n",
      "Test Loss: 1.5853 | Test Acc: 0.6063\n",
      "Epoch 71 took 4.55 seconds\n",
      "Epoch [72/100] | Train Loss: 0.0050 | Train Acc: 1.0000\n",
      "Test Loss: 1.4917 | Test Acc: 0.6122\n",
      "Epoch 72 took 4.54 seconds\n",
      "Epoch [73/100] | Train Loss: 0.0047 | Train Acc: 1.0000\n",
      "Test Loss: 1.5041 | Test Acc: 0.6098\n",
      "Epoch 73 took 4.51 seconds\n",
      "Epoch [74/100] | Train Loss: 0.0046 | Train Acc: 1.0000\n",
      "Test Loss: 1.4961 | Test Acc: 0.6136\n",
      "Epoch 74 took 4.55 seconds\n",
      "Epoch [75/100] | Train Loss: 0.0046 | Train Acc: 1.0000\n",
      "Test Loss: 1.4729 | Test Acc: 0.6176\n",
      "Epoch 75 took 4.54 seconds\n",
      "Epoch [76/100] | Train Loss: 0.0048 | Train Acc: 1.0000\n",
      "Test Loss: 1.4920 | Test Acc: 0.6138\n",
      "Epoch 76 took 4.53 seconds\n",
      "Epoch [77/100] | Train Loss: 0.0044 | Train Acc: 1.0000\n",
      "Test Loss: 1.5151 | Test Acc: 0.6118\n",
      "Epoch 77 took 4.57 seconds\n",
      "Epoch [78/100] | Train Loss: 0.0044 | Train Acc: 1.0000\n",
      "Test Loss: 1.4727 | Test Acc: 0.6143\n",
      "Epoch 78 took 4.53 seconds\n",
      "Epoch [79/100] | Train Loss: 0.0043 | Train Acc: 1.0000\n",
      "Test Loss: 1.5335 | Test Acc: 0.6071\n",
      "Epoch 79 took 4.53 seconds\n",
      "Epoch [80/100] | Train Loss: 0.0046 | Train Acc: 1.0000\n",
      "Test Loss: 1.5639 | Test Acc: 0.6084\n",
      "Epoch 80 took 4.51 seconds\n",
      "Epoch [81/100] | Train Loss: 0.0043 | Train Acc: 1.0000\n",
      "Test Loss: 1.5057 | Test Acc: 0.6142\n",
      "Epoch 81 took 4.50 seconds\n",
      "Epoch [82/100] | Train Loss: 0.0042 | Train Acc: 1.0000\n",
      "Test Loss: 1.5076 | Test Acc: 0.6152\n",
      "Epoch 82 took 4.52 seconds\n",
      "Epoch [83/100] | Train Loss: 0.0043 | Train Acc: 1.0000\n",
      "Test Loss: 1.6378 | Test Acc: 0.5975\n",
      "Epoch 83 took 4.52 seconds\n",
      "Epoch [84/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.5481 | Test Acc: 0.6117\n",
      "Epoch 84 took 4.56 seconds\n",
      "Epoch [85/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.5973 | Test Acc: 0.6028\n",
      "Epoch 85 took 4.53 seconds\n",
      "Epoch [86/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.4956 | Test Acc: 0.6154\n",
      "Epoch 86 took 4.52 seconds\n",
      "Epoch [87/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.5412 | Test Acc: 0.6111\n",
      "Epoch 87 took 4.54 seconds\n",
      "Epoch [88/100] | Train Loss: 0.0038 | Train Acc: 1.0000\n",
      "Test Loss: 1.5524 | Test Acc: 0.6118\n",
      "Epoch 88 took 4.53 seconds\n",
      "Epoch [89/100] | Train Loss: 0.0038 | Train Acc: 1.0000\n",
      "Test Loss: 1.5556 | Test Acc: 0.6108\n",
      "Epoch 89 took 4.55 seconds\n",
      "Epoch [90/100] | Train Loss: 0.0043 | Train Acc: 0.9999\n",
      "Test Loss: 1.6448 | Test Acc: 0.5928\n",
      "Epoch 90 took 4.56 seconds\n",
      "Epoch [91/100] | Train Loss: 0.0041 | Train Acc: 1.0000\n",
      "Test Loss: 1.5958 | Test Acc: 0.6033\n",
      "Epoch 91 took 4.50 seconds\n",
      "Epoch [92/100] | Train Loss: 0.0038 | Train Acc: 1.0000\n",
      "Test Loss: 1.5049 | Test Acc: 0.6161\n",
      "Epoch 92 took 4.53 seconds\n",
      "Epoch [93/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.5446 | Test Acc: 0.6102\n",
      "Epoch 93 took 4.50 seconds\n",
      "Epoch [94/100] | Train Loss: 0.0036 | Train Acc: 1.0000\n",
      "Test Loss: 1.5399 | Test Acc: 0.6137\n",
      "Epoch 94 took 4.52 seconds\n",
      "Epoch [95/100] | Train Loss: 0.0037 | Train Acc: 1.0000\n",
      "Test Loss: 1.5342 | Test Acc: 0.6092\n",
      "Epoch 95 took 4.53 seconds\n",
      "Epoch [96/100] | Train Loss: 0.0036 | Train Acc: 1.0000\n",
      "Test Loss: 1.4933 | Test Acc: 0.6135\n",
      "Epoch 96 took 4.52 seconds\n",
      "Epoch [97/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.4883 | Test Acc: 0.6145\n",
      "Epoch 97 took 4.52 seconds\n",
      "Epoch [98/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5900 | Test Acc: 0.6084\n",
      "Epoch 98 took 4.52 seconds\n",
      "Epoch [99/100] | Train Loss: 0.0036 | Train Acc: 1.0000\n",
      "Test Loss: 1.5307 | Test Acc: 0.6147\n",
      "Epoch 99 took 4.52 seconds\n",
      "Epoch [100/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5617 | Test Acc: 0.6139\n",
      "Epoch 100 took 4.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:50:13,039 - INFO - Train accuracy 1.0, Train Loss 0.0030063338630015447\n",
      "2025-02-12 15:50:13,041 - INFO - Test accuracy 0.61392, Test Loss 1.5615280221919625\n",
      "2025-02-12 15:50:13,056 - INFO - Training model 0 took 455.60379457473755 seconds\n",
      "2025-02-12 15:50:13,073 - INFO - --------------------------------------------------\n",
      "2025-02-12 15:50:13,074 - INFO - Training model 1: Train size 25000, Test size 25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimizer: SGD | Learning Rate: 0.1 | Weight Decay: 0\n",
      "Epoch [1/100] | Train Loss: 2.2376 | Train Acc: 0.1516\n",
      "Test Loss: 2.0878 | Test Acc: 0.2406\n",
      "Epoch 1 took 4.74 seconds\n",
      "Epoch [2/100] | Train Loss: 1.9509 | Train Acc: 0.2836\n",
      "Test Loss: 1.8576 | Test Acc: 0.3239\n",
      "Epoch 2 took 4.52 seconds\n",
      "Epoch [3/100] | Train Loss: 1.7912 | Train Acc: 0.3412\n",
      "Test Loss: 1.7769 | Test Acc: 0.3447\n",
      "Epoch 3 took 4.51 seconds\n",
      "Epoch [4/100] | Train Loss: 1.6763 | Train Acc: 0.3779\n",
      "Test Loss: 1.6298 | Test Acc: 0.4056\n",
      "Epoch 4 took 4.51 seconds\n",
      "Epoch [5/100] | Train Loss: 1.5652 | Train Acc: 0.4259\n",
      "Test Loss: 1.5369 | Test Acc: 0.4376\n",
      "Epoch 5 took 4.53 seconds\n",
      "Epoch [6/100] | Train Loss: 1.4787 | Train Acc: 0.4556\n",
      "Test Loss: 1.4494 | Test Acc: 0.4660\n",
      "Epoch 6 took 4.51 seconds\n",
      "Epoch [7/100] | Train Loss: 1.3972 | Train Acc: 0.4909\n",
      "Test Loss: 1.4280 | Test Acc: 0.4776\n",
      "Epoch 7 took 4.52 seconds\n",
      "Epoch [8/100] | Train Loss: 1.3210 | Train Acc: 0.5192\n",
      "Test Loss: 1.5315 | Test Acc: 0.4263\n",
      "Epoch 8 took 4.55 seconds\n",
      "Epoch [9/100] | Train Loss: 1.2524 | Train Acc: 0.5480\n",
      "Test Loss: 1.4194 | Test Acc: 0.4739\n",
      "Epoch 9 took 4.51 seconds\n",
      "Epoch [10/100] | Train Loss: 1.1892 | Train Acc: 0.5701\n",
      "Test Loss: 1.6115 | Test Acc: 0.4386\n",
      "Epoch 10 took 4.53 seconds\n",
      "Epoch [11/100] | Train Loss: 1.1355 | Train Acc: 0.5928\n",
      "Test Loss: 1.4394 | Test Acc: 0.4896\n",
      "Epoch 11 took 4.52 seconds\n",
      "Epoch [12/100] | Train Loss: 1.0772 | Train Acc: 0.6130\n",
      "Test Loss: 1.2831 | Test Acc: 0.5362\n",
      "Epoch 12 took 4.53 seconds\n",
      "Epoch [13/100] | Train Loss: 1.0298 | Train Acc: 0.6358\n",
      "Test Loss: 1.2169 | Test Acc: 0.5555\n",
      "Epoch 13 took 4.55 seconds\n",
      "Epoch [14/100] | Train Loss: 0.9776 | Train Acc: 0.6510\n",
      "Test Loss: 1.3579 | Test Acc: 0.5164\n",
      "Epoch 14 took 4.58 seconds\n",
      "Epoch [15/100] | Train Loss: 0.9322 | Train Acc: 0.6728\n",
      "Test Loss: 1.2163 | Test Acc: 0.5552\n",
      "Epoch 15 took 4.53 seconds\n",
      "Epoch [16/100] | Train Loss: 0.8879 | Train Acc: 0.6885\n",
      "Test Loss: 1.1948 | Test Acc: 0.5772\n",
      "Epoch 16 took 4.53 seconds\n",
      "Epoch [17/100] | Train Loss: 0.8534 | Train Acc: 0.7002\n",
      "Test Loss: 1.9247 | Test Acc: 0.4213\n",
      "Epoch 17 took 4.52 seconds\n",
      "Epoch [18/100] | Train Loss: 0.7992 | Train Acc: 0.7246\n",
      "Test Loss: 1.0829 | Test Acc: 0.6139\n",
      "Epoch 18 took 4.53 seconds\n",
      "Epoch [19/100] | Train Loss: 0.7569 | Train Acc: 0.7353\n",
      "Test Loss: 1.8586 | Test Acc: 0.4462\n",
      "Epoch 19 took 4.52 seconds\n",
      "Epoch [20/100] | Train Loss: 0.7078 | Train Acc: 0.7546\n",
      "Test Loss: 1.5387 | Test Acc: 0.5065\n",
      "Epoch 20 took 4.51 seconds\n",
      "Epoch [21/100] | Train Loss: 0.6712 | Train Acc: 0.7729\n",
      "Test Loss: 1.7580 | Test Acc: 0.4952\n",
      "Epoch 21 took 4.52 seconds\n",
      "Epoch [22/100] | Train Loss: 0.6235 | Train Acc: 0.7896\n",
      "Test Loss: 1.3687 | Test Acc: 0.5498\n",
      "Epoch 22 took 4.51 seconds\n",
      "Epoch [23/100] | Train Loss: 0.5803 | Train Acc: 0.8056\n",
      "Test Loss: 1.5718 | Test Acc: 0.5077\n",
      "Epoch 23 took 4.52 seconds\n",
      "Epoch [24/100] | Train Loss: 0.5259 | Train Acc: 0.8276\n",
      "Test Loss: 1.3860 | Test Acc: 0.5540\n",
      "Epoch 24 took 4.51 seconds\n",
      "Epoch [25/100] | Train Loss: 0.4917 | Train Acc: 0.8366\n",
      "Test Loss: 1.2568 | Test Acc: 0.5830\n",
      "Epoch 25 took 4.49 seconds\n",
      "Epoch [26/100] | Train Loss: 0.4436 | Train Acc: 0.8582\n",
      "Test Loss: 1.6281 | Test Acc: 0.5136\n",
      "Epoch 26 took 4.58 seconds\n",
      "Epoch [27/100] | Train Loss: 0.3978 | Train Acc: 0.8764\n",
      "Test Loss: 4.5209 | Test Acc: 0.3239\n",
      "Epoch 27 took 4.51 seconds\n",
      "Epoch [28/100] | Train Loss: 0.3717 | Train Acc: 0.8826\n",
      "Test Loss: 1.5392 | Test Acc: 0.5374\n",
      "Epoch 28 took 4.51 seconds\n",
      "Epoch [29/100] | Train Loss: 0.3063 | Train Acc: 0.9093\n",
      "Test Loss: 3.7981 | Test Acc: 0.3400\n",
      "Epoch 29 took 4.52 seconds\n",
      "Epoch [30/100] | Train Loss: 0.2756 | Train Acc: 0.9204\n",
      "Test Loss: 2.0437 | Test Acc: 0.4987\n",
      "Epoch 30 took 4.50 seconds\n",
      "Epoch [31/100] | Train Loss: 0.2374 | Train Acc: 0.9364\n",
      "Test Loss: 2.1846 | Test Acc: 0.4378\n",
      "Epoch 31 took 4.52 seconds\n",
      "Epoch [32/100] | Train Loss: 0.2022 | Train Acc: 0.9485\n",
      "Test Loss: 1.8231 | Test Acc: 0.5315\n",
      "Epoch 32 took 4.53 seconds\n",
      "Epoch [33/100] | Train Loss: 0.1550 | Train Acc: 0.9650\n",
      "Test Loss: 1.3796 | Test Acc: 0.6022\n",
      "Epoch 33 took 4.55 seconds\n",
      "Epoch [34/100] | Train Loss: 0.1052 | Train Acc: 0.9844\n",
      "Test Loss: 1.2722 | Test Acc: 0.6241\n",
      "Epoch 34 took 4.51 seconds\n",
      "Epoch [35/100] | Train Loss: 0.0803 | Train Acc: 0.9900\n",
      "Test Loss: 1.2957 | Test Acc: 0.6214\n",
      "Epoch 35 took 4.52 seconds\n",
      "Epoch [36/100] | Train Loss: 0.0652 | Train Acc: 0.9935\n",
      "Test Loss: 1.4481 | Test Acc: 0.5966\n",
      "Epoch 36 took 4.50 seconds\n",
      "Epoch [37/100] | Train Loss: 0.0452 | Train Acc: 0.9976\n",
      "Test Loss: 1.4133 | Test Acc: 0.6102\n",
      "Epoch 37 took 4.51 seconds\n",
      "Epoch [38/100] | Train Loss: 0.0362 | Train Acc: 0.9988\n",
      "Test Loss: 1.4375 | Test Acc: 0.5976\n",
      "Epoch 38 took 4.51 seconds\n",
      "Epoch [39/100] | Train Loss: 0.0291 | Train Acc: 0.9994\n",
      "Test Loss: 1.3425 | Test Acc: 0.6246\n",
      "Epoch 39 took 4.51 seconds\n",
      "Epoch [40/100] | Train Loss: 0.0246 | Train Acc: 0.9997\n",
      "Test Loss: 1.5333 | Test Acc: 0.5981\n",
      "Epoch 40 took 4.52 seconds\n",
      "Epoch [41/100] | Train Loss: 0.0201 | Train Acc: 0.9998\n",
      "Test Loss: 1.3013 | Test Acc: 0.6315\n",
      "Epoch 41 took 4.51 seconds\n",
      "Epoch [42/100] | Train Loss: 0.0186 | Train Acc: 0.9999\n",
      "Test Loss: 1.7492 | Test Acc: 0.5617\n",
      "Epoch 42 took 4.51 seconds\n",
      "Epoch [43/100] | Train Loss: 0.0165 | Train Acc: 0.9999\n",
      "Test Loss: 1.6377 | Test Acc: 0.5864\n",
      "Epoch 43 took 4.51 seconds\n",
      "Epoch [44/100] | Train Loss: 0.0145 | Train Acc: 1.0000\n",
      "Test Loss: 1.3292 | Test Acc: 0.6299\n",
      "Epoch 44 took 4.51 seconds\n",
      "Epoch [45/100] | Train Loss: 0.0136 | Train Acc: 1.0000\n",
      "Test Loss: 1.3913 | Test Acc: 0.6286\n",
      "Epoch 45 took 4.54 seconds\n",
      "Epoch [46/100] | Train Loss: 0.0124 | Train Acc: 1.0000\n",
      "Test Loss: 1.3914 | Test Acc: 0.6257\n",
      "Epoch 46 took 4.50 seconds\n",
      "Epoch [47/100] | Train Loss: 0.0118 | Train Acc: 1.0000\n",
      "Test Loss: 1.3574 | Test Acc: 0.6294\n",
      "Epoch 47 took 4.53 seconds\n",
      "Epoch [48/100] | Train Loss: 0.0107 | Train Acc: 1.0000\n",
      "Test Loss: 1.3804 | Test Acc: 0.6294\n",
      "Epoch 48 took 4.56 seconds\n",
      "Epoch [49/100] | Train Loss: 0.0106 | Train Acc: 1.0000\n",
      "Test Loss: 1.4418 | Test Acc: 0.6189\n",
      "Epoch 49 took 4.57 seconds\n",
      "Epoch [50/100] | Train Loss: 0.0099 | Train Acc: 1.0000\n",
      "Test Loss: 1.4830 | Test Acc: 0.6193\n",
      "Epoch 50 took 4.52 seconds\n",
      "Epoch [51/100] | Train Loss: 0.0098 | Train Acc: 1.0000\n",
      "Test Loss: 1.4769 | Test Acc: 0.6237\n",
      "Epoch 51 took 4.57 seconds\n",
      "Epoch [52/100] | Train Loss: 0.0088 | Train Acc: 1.0000\n",
      "Test Loss: 1.4262 | Test Acc: 0.6289\n",
      "Epoch 52 took 4.53 seconds\n",
      "Epoch [53/100] | Train Loss: 0.0082 | Train Acc: 1.0000\n",
      "Test Loss: 1.4166 | Test Acc: 0.6272\n",
      "Epoch 53 took 4.52 seconds\n",
      "Epoch [54/100] | Train Loss: 0.0082 | Train Acc: 1.0000\n",
      "Test Loss: 1.4042 | Test Acc: 0.6273\n",
      "Epoch 54 took 4.51 seconds\n",
      "Epoch [55/100] | Train Loss: 0.0083 | Train Acc: 1.0000\n",
      "Test Loss: 1.4202 | Test Acc: 0.6258\n",
      "Epoch 55 took 4.52 seconds\n",
      "Epoch [56/100] | Train Loss: 0.0079 | Train Acc: 1.0000\n",
      "Test Loss: 1.5014 | Test Acc: 0.6178\n",
      "Epoch 56 took 4.53 seconds\n",
      "Epoch [57/100] | Train Loss: 0.0074 | Train Acc: 1.0000\n",
      "Test Loss: 1.4923 | Test Acc: 0.6138\n",
      "Epoch 57 took 4.51 seconds\n",
      "Epoch [58/100] | Train Loss: 0.0070 | Train Acc: 1.0000\n",
      "Test Loss: 1.4330 | Test Acc: 0.6275\n",
      "Epoch 58 took 4.54 seconds\n",
      "Epoch [59/100] | Train Loss: 0.0068 | Train Acc: 1.0000\n",
      "Test Loss: 1.4682 | Test Acc: 0.6215\n",
      "Epoch 59 took 4.51 seconds\n",
      "Epoch [60/100] | Train Loss: 0.0065 | Train Acc: 1.0000\n",
      "Test Loss: 1.4912 | Test Acc: 0.6230\n",
      "Epoch 60 took 4.53 seconds\n",
      "Epoch [61/100] | Train Loss: 0.0063 | Train Acc: 1.0000\n",
      "Test Loss: 1.4464 | Test Acc: 0.6244\n",
      "Epoch 61 took 4.50 seconds\n",
      "Epoch [62/100] | Train Loss: 0.0063 | Train Acc: 1.0000\n",
      "Test Loss: 1.6129 | Test Acc: 0.6020\n",
      "Epoch 62 took 4.52 seconds\n",
      "Epoch [63/100] | Train Loss: 0.0060 | Train Acc: 1.0000\n",
      "Test Loss: 1.4593 | Test Acc: 0.6275\n",
      "Epoch 63 took 4.53 seconds\n",
      "Epoch [64/100] | Train Loss: 0.0060 | Train Acc: 1.0000\n",
      "Test Loss: 1.4639 | Test Acc: 0.6257\n",
      "Epoch 64 took 4.51 seconds\n",
      "Epoch [65/100] | Train Loss: 0.0058 | Train Acc: 1.0000\n",
      "Test Loss: 1.4786 | Test Acc: 0.6230\n",
      "Epoch 65 took 4.54 seconds\n",
      "Epoch [66/100] | Train Loss: 0.0056 | Train Acc: 1.0000\n",
      "Test Loss: 1.5113 | Test Acc: 0.6219\n",
      "Epoch 66 took 4.50 seconds\n",
      "Epoch [67/100] | Train Loss: 0.0055 | Train Acc: 1.0000\n",
      "Test Loss: 1.4728 | Test Acc: 0.6228\n",
      "Epoch 67 took 4.50 seconds\n",
      "Epoch [68/100] | Train Loss: 0.0052 | Train Acc: 1.0000\n",
      "Test Loss: 1.4523 | Test Acc: 0.6300\n",
      "Epoch 68 took 4.54 seconds\n",
      "Epoch [69/100] | Train Loss: 0.0051 | Train Acc: 1.0000\n",
      "Test Loss: 1.4842 | Test Acc: 0.6246\n",
      "Epoch 69 took 4.49 seconds\n",
      "Epoch [70/100] | Train Loss: 0.0053 | Train Acc: 1.0000\n",
      "Test Loss: 1.4638 | Test Acc: 0.6284\n",
      "Epoch 70 took 4.50 seconds\n",
      "Epoch [71/100] | Train Loss: 0.0050 | Train Acc: 1.0000\n",
      "Test Loss: 1.4471 | Test Acc: 0.6258\n",
      "Epoch 71 took 4.56 seconds\n",
      "Epoch [72/100] | Train Loss: 0.0050 | Train Acc: 1.0000\n",
      "Test Loss: 1.4805 | Test Acc: 0.6234\n",
      "Epoch 72 took 4.51 seconds\n",
      "Epoch [73/100] | Train Loss: 0.0047 | Train Acc: 1.0000\n",
      "Test Loss: 1.4881 | Test Acc: 0.6257\n",
      "Epoch 73 took 4.52 seconds\n",
      "Epoch [74/100] | Train Loss: 0.0045 | Train Acc: 1.0000\n",
      "Test Loss: 1.5087 | Test Acc: 0.6194\n",
      "Epoch 74 took 4.54 seconds\n",
      "Epoch [75/100] | Train Loss: 0.0046 | Train Acc: 1.0000\n",
      "Test Loss: 1.4891 | Test Acc: 0.6241\n",
      "Epoch 75 took 4.50 seconds\n",
      "Epoch [76/100] | Train Loss: 0.0048 | Train Acc: 1.0000\n",
      "Test Loss: 1.5018 | Test Acc: 0.6211\n",
      "Epoch 76 took 4.52 seconds\n",
      "Epoch [77/100] | Train Loss: 0.0045 | Train Acc: 1.0000\n",
      "Test Loss: 1.4775 | Test Acc: 0.6224\n",
      "Epoch 77 took 4.52 seconds\n",
      "Epoch [78/100] | Train Loss: 0.0044 | Train Acc: 1.0000\n",
      "Test Loss: 1.5214 | Test Acc: 0.6221\n",
      "Epoch 78 took 4.55 seconds\n",
      "Epoch [79/100] | Train Loss: 0.0042 | Train Acc: 1.0000\n",
      "Test Loss: 1.4702 | Test Acc: 0.6240\n",
      "Epoch 79 took 4.53 seconds\n",
      "Epoch [80/100] | Train Loss: 0.0043 | Train Acc: 1.0000\n",
      "Test Loss: 1.5600 | Test Acc: 0.6188\n",
      "Epoch 80 took 4.51 seconds\n",
      "Epoch [81/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.4763 | Test Acc: 0.6265\n",
      "Epoch 81 took 4.51 seconds\n",
      "Epoch [82/100] | Train Loss: 0.0041 | Train Acc: 1.0000\n",
      "Test Loss: 1.4832 | Test Acc: 0.6272\n",
      "Epoch 82 took 4.52 seconds\n",
      "Epoch [83/100] | Train Loss: 0.0041 | Train Acc: 1.0000\n",
      "Test Loss: 1.5056 | Test Acc: 0.6259\n",
      "Epoch 83 took 4.51 seconds\n",
      "Epoch [84/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.5433 | Test Acc: 0.6231\n",
      "Epoch 84 took 4.53 seconds\n",
      "Epoch [85/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.5068 | Test Acc: 0.6236\n",
      "Epoch 85 took 4.52 seconds\n",
      "Epoch [86/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4890 | Test Acc: 0.6264\n",
      "Epoch 86 took 4.51 seconds\n",
      "Epoch [87/100] | Train Loss: 0.0038 | Train Acc: 1.0000\n",
      "Test Loss: 1.5340 | Test Acc: 0.6231\n",
      "Epoch 87 took 4.53 seconds\n",
      "Epoch [88/100] | Train Loss: 0.0038 | Train Acc: 1.0000\n",
      "Test Loss: 1.5279 | Test Acc: 0.6198\n",
      "Epoch 88 took 4.52 seconds\n",
      "Epoch [89/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4969 | Test Acc: 0.6254\n",
      "Epoch 89 took 4.52 seconds\n",
      "Epoch [90/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4973 | Test Acc: 0.6272\n",
      "Epoch 90 took 4.51 seconds\n",
      "Epoch [91/100] | Train Loss: 0.0037 | Train Acc: 1.0000\n",
      "Test Loss: 1.5086 | Test Acc: 0.6270\n",
      "Epoch 91 took 4.49 seconds\n",
      "Epoch [92/100] | Train Loss: 0.0037 | Train Acc: 1.0000\n",
      "Test Loss: 1.5957 | Test Acc: 0.6187\n",
      "Epoch 92 took 4.55 seconds\n",
      "Epoch [93/100] | Train Loss: 0.0037 | Train Acc: 1.0000\n",
      "Test Loss: 1.5252 | Test Acc: 0.6239\n",
      "Epoch 93 took 4.53 seconds\n",
      "Epoch [94/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5202 | Test Acc: 0.6251\n",
      "Epoch 94 took 4.53 seconds\n",
      "Epoch [95/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5122 | Test Acc: 0.6223\n",
      "Epoch 95 took 4.53 seconds\n",
      "Epoch [96/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.4995 | Test Acc: 0.6235\n",
      "Epoch 96 took 4.51 seconds\n",
      "Epoch [97/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5465 | Test Acc: 0.6208\n",
      "Epoch 97 took 4.51 seconds\n",
      "Epoch [98/100] | Train Loss: 0.0036 | Train Acc: 1.0000\n",
      "Test Loss: 1.5616 | Test Acc: 0.6201\n",
      "Epoch 98 took 4.52 seconds\n",
      "Epoch [99/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5389 | Test Acc: 0.6246\n",
      "Epoch 99 took 4.52 seconds\n",
      "Epoch [100/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.5137 | Test Acc: 0.6243\n",
      "Epoch 100 took 4.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:57:47,811 - INFO - Train accuracy 1.0, Train Loss 0.003254153979562071\n",
      "2025-02-12 15:57:47,812 - INFO - Test accuracy 0.62428, Test Loss 1.5147887894085474\n",
      "2025-02-12 15:57:47,837 - INFO - Training model 1 took 454.7633502483368 seconds\n",
      "2025-02-12 15:57:47,857 - INFO - --------------------------------------------------\n",
      "2025-02-12 15:57:47,858 - INFO - Training model 2: Train size 25000, Test size 25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimizer: SGD | Learning Rate: 0.1 | Weight Decay: 0\n",
      "Epoch [1/100] | Train Loss: 2.2637 | Train Acc: 0.1442\n",
      "Test Loss: 2.1554 | Test Acc: 0.2340\n",
      "Epoch 1 took 4.78 seconds\n",
      "Epoch [2/100] | Train Loss: 2.0124 | Train Acc: 0.2791\n",
      "Test Loss: 1.8895 | Test Acc: 0.3175\n",
      "Epoch 2 took 4.53 seconds\n",
      "Epoch [3/100] | Train Loss: 1.8021 | Train Acc: 0.3520\n",
      "Test Loss: 1.7632 | Test Acc: 0.3630\n",
      "Epoch 3 took 4.52 seconds\n",
      "Epoch [4/100] | Train Loss: 1.6473 | Train Acc: 0.4060\n",
      "Test Loss: 1.5997 | Test Acc: 0.4174\n",
      "Epoch 4 took 4.51 seconds\n",
      "Epoch [5/100] | Train Loss: 1.5365 | Train Acc: 0.4423\n",
      "Test Loss: 1.6156 | Test Acc: 0.4161\n",
      "Epoch 5 took 4.51 seconds\n",
      "Epoch [6/100] | Train Loss: 1.4412 | Train Acc: 0.4810\n",
      "Test Loss: 1.4512 | Test Acc: 0.4682\n",
      "Epoch 6 took 4.53 seconds\n",
      "Epoch [7/100] | Train Loss: 1.3641 | Train Acc: 0.5070\n",
      "Test Loss: 1.5189 | Test Acc: 0.4428\n",
      "Epoch 7 took 4.51 seconds\n",
      "Epoch [8/100] | Train Loss: 1.2945 | Train Acc: 0.5332\n",
      "Test Loss: 1.7329 | Test Acc: 0.4188\n",
      "Epoch 8 took 4.51 seconds\n",
      "Epoch [9/100] | Train Loss: 1.2308 | Train Acc: 0.5565\n",
      "Test Loss: 1.3385 | Test Acc: 0.5074\n",
      "Epoch 9 took 4.52 seconds\n",
      "Epoch [10/100] | Train Loss: 1.1758 | Train Acc: 0.5818\n",
      "Test Loss: 1.3245 | Test Acc: 0.5080\n",
      "Epoch 10 took 4.52 seconds\n",
      "Epoch [11/100] | Train Loss: 1.1130 | Train Acc: 0.6044\n",
      "Test Loss: 1.3653 | Test Acc: 0.4994\n",
      "Epoch 11 took 4.53 seconds\n",
      "Epoch [12/100] | Train Loss: 1.0689 | Train Acc: 0.6182\n",
      "Test Loss: 1.5847 | Test Acc: 0.4284\n",
      "Epoch 12 took 4.54 seconds\n",
      "Epoch [13/100] | Train Loss: 1.0180 | Train Acc: 0.6395\n",
      "Test Loss: 1.6505 | Test Acc: 0.4682\n",
      "Epoch 13 took 4.54 seconds\n",
      "Epoch [14/100] | Train Loss: 0.9698 | Train Acc: 0.6604\n",
      "Test Loss: 1.3274 | Test Acc: 0.5274\n",
      "Epoch 14 took 4.51 seconds\n",
      "Epoch [15/100] | Train Loss: 0.9225 | Train Acc: 0.6772\n",
      "Test Loss: 1.2831 | Test Acc: 0.5568\n",
      "Epoch 15 took 4.53 seconds\n",
      "Epoch [16/100] | Train Loss: 0.8828 | Train Acc: 0.6887\n",
      "Test Loss: 1.3218 | Test Acc: 0.5410\n",
      "Epoch 16 took 4.51 seconds\n",
      "Epoch [17/100] | Train Loss: 0.8275 | Train Acc: 0.7138\n",
      "Test Loss: 1.5372 | Test Acc: 0.4866\n",
      "Epoch 17 took 4.50 seconds\n",
      "Epoch [18/100] | Train Loss: 0.7852 | Train Acc: 0.7280\n",
      "Test Loss: 1.7396 | Test Acc: 0.4612\n",
      "Epoch 18 took 4.53 seconds\n",
      "Epoch [19/100] | Train Loss: 0.7576 | Train Acc: 0.7357\n",
      "Test Loss: 1.7268 | Test Acc: 0.4536\n",
      "Epoch 19 took 4.53 seconds\n",
      "Epoch [20/100] | Train Loss: 0.7025 | Train Acc: 0.7600\n",
      "Test Loss: 2.1696 | Test Acc: 0.4374\n",
      "Epoch 20 took 4.54 seconds\n",
      "Epoch [21/100] | Train Loss: 0.6498 | Train Acc: 0.7827\n",
      "Test Loss: 1.5065 | Test Acc: 0.5420\n",
      "Epoch 21 took 4.53 seconds\n",
      "Epoch [22/100] | Train Loss: 0.6009 | Train Acc: 0.7998\n",
      "Test Loss: 1.8408 | Test Acc: 0.4898\n",
      "Epoch 22 took 4.53 seconds\n",
      "Epoch [23/100] | Train Loss: 0.5601 | Train Acc: 0.8134\n",
      "Test Loss: 2.9217 | Test Acc: 0.3514\n",
      "Epoch 23 took 4.53 seconds\n",
      "Epoch [24/100] | Train Loss: 0.5144 | Train Acc: 0.8322\n",
      "Test Loss: 1.7181 | Test Acc: 0.4886\n",
      "Epoch 24 took 4.54 seconds\n",
      "Epoch [25/100] | Train Loss: 0.4749 | Train Acc: 0.8459\n",
      "Test Loss: 1.9067 | Test Acc: 0.4824\n",
      "Epoch 25 took 4.53 seconds\n",
      "Epoch [26/100] | Train Loss: 0.4234 | Train Acc: 0.8664\n",
      "Test Loss: 1.8216 | Test Acc: 0.4990\n",
      "Epoch 26 took 4.53 seconds\n",
      "Epoch [27/100] | Train Loss: 0.3881 | Train Acc: 0.8787\n",
      "Test Loss: 2.0605 | Test Acc: 0.4743\n",
      "Epoch 27 took 4.53 seconds\n",
      "Epoch [28/100] | Train Loss: 0.3249 | Train Acc: 0.9048\n",
      "Test Loss: 1.7630 | Test Acc: 0.5224\n",
      "Epoch 28 took 4.52 seconds\n",
      "Epoch [29/100] | Train Loss: 0.2779 | Train Acc: 0.9217\n",
      "Test Loss: 1.6328 | Test Acc: 0.5460\n",
      "Epoch 29 took 4.53 seconds\n",
      "Epoch [30/100] | Train Loss: 0.2637 | Train Acc: 0.9258\n",
      "Test Loss: 2.1602 | Test Acc: 0.4745\n",
      "Epoch 30 took 4.51 seconds\n",
      "Epoch [31/100] | Train Loss: 0.2064 | Train Acc: 0.9476\n",
      "Test Loss: 3.5656 | Test Acc: 0.4275\n",
      "Epoch 31 took 4.52 seconds\n",
      "Epoch [32/100] | Train Loss: 0.1503 | Train Acc: 0.9685\n",
      "Test Loss: 1.7529 | Test Acc: 0.5374\n",
      "Epoch 32 took 4.57 seconds\n",
      "Epoch [33/100] | Train Loss: 0.1048 | Train Acc: 0.9828\n",
      "Test Loss: 1.2882 | Test Acc: 0.6228\n",
      "Epoch 33 took 4.50 seconds\n",
      "Epoch [34/100] | Train Loss: 0.0766 | Train Acc: 0.9909\n",
      "Test Loss: 1.3157 | Test Acc: 0.6231\n",
      "Epoch 34 took 4.51 seconds\n",
      "Epoch [35/100] | Train Loss: 0.0640 | Train Acc: 0.9936\n",
      "Test Loss: 1.4939 | Test Acc: 0.5897\n",
      "Epoch 35 took 4.52 seconds\n",
      "Epoch [36/100] | Train Loss: 0.0477 | Train Acc: 0.9965\n",
      "Test Loss: 1.3228 | Test Acc: 0.6272\n",
      "Epoch 36 took 4.52 seconds\n",
      "Epoch [37/100] | Train Loss: 0.0382 | Train Acc: 0.9980\n",
      "Test Loss: 1.4146 | Test Acc: 0.6185\n",
      "Epoch 37 took 4.51 seconds\n",
      "Epoch [38/100] | Train Loss: 0.0307 | Train Acc: 0.9984\n",
      "Test Loss: 1.3224 | Test Acc: 0.6278\n",
      "Epoch 38 took 4.51 seconds\n",
      "Epoch [39/100] | Train Loss: 0.0260 | Train Acc: 0.9988\n",
      "Test Loss: 1.8718 | Test Acc: 0.5448\n",
      "Epoch 39 took 4.51 seconds\n",
      "Epoch [40/100] | Train Loss: 0.0238 | Train Acc: 0.9992\n",
      "Test Loss: 1.3044 | Test Acc: 0.6315\n",
      "Epoch 40 took 4.51 seconds\n",
      "Epoch [41/100] | Train Loss: 0.0202 | Train Acc: 0.9994\n",
      "Test Loss: 1.2910 | Test Acc: 0.6365\n",
      "Epoch 41 took 4.50 seconds\n",
      "Epoch [42/100] | Train Loss: 0.0182 | Train Acc: 0.9995\n",
      "Test Loss: 1.3276 | Test Acc: 0.6397\n",
      "Epoch 42 took 4.51 seconds\n",
      "Epoch [43/100] | Train Loss: 0.0156 | Train Acc: 0.9997\n",
      "Test Loss: 1.4218 | Test Acc: 0.6197\n",
      "Epoch 43 took 4.51 seconds\n",
      "Epoch [44/100] | Train Loss: 0.0152 | Train Acc: 0.9996\n",
      "Test Loss: 1.3555 | Test Acc: 0.6364\n",
      "Epoch 44 took 4.52 seconds\n",
      "Epoch [45/100] | Train Loss: 0.0135 | Train Acc: 0.9997\n",
      "Test Loss: 1.3382 | Test Acc: 0.6355\n",
      "Epoch 45 took 4.54 seconds\n",
      "Epoch [46/100] | Train Loss: 0.0122 | Train Acc: 0.9998\n",
      "Test Loss: 1.3714 | Test Acc: 0.6339\n",
      "Epoch 46 took 4.52 seconds\n",
      "Epoch [47/100] | Train Loss: 0.0112 | Train Acc: 0.9998\n",
      "Test Loss: 1.3319 | Test Acc: 0.6414\n",
      "Epoch 47 took 4.53 seconds\n",
      "Epoch [48/100] | Train Loss: 0.0105 | Train Acc: 0.9999\n",
      "Test Loss: 1.3432 | Test Acc: 0.6372\n",
      "Epoch 48 took 4.52 seconds\n",
      "Epoch [49/100] | Train Loss: 0.0096 | Train Acc: 0.9999\n",
      "Test Loss: 1.3228 | Test Acc: 0.6382\n",
      "Epoch 49 took 4.56 seconds\n",
      "Epoch [50/100] | Train Loss: 0.0092 | Train Acc: 0.9999\n",
      "Test Loss: 1.3290 | Test Acc: 0.6416\n",
      "Epoch 50 took 4.52 seconds\n",
      "Epoch [51/100] | Train Loss: 0.0089 | Train Acc: 0.9999\n",
      "Test Loss: 1.3701 | Test Acc: 0.6334\n",
      "Epoch 51 took 4.52 seconds\n",
      "Epoch [52/100] | Train Loss: 0.0085 | Train Acc: 0.9999\n",
      "Test Loss: 1.3607 | Test Acc: 0.6357\n",
      "Epoch 52 took 4.51 seconds\n",
      "Epoch [53/100] | Train Loss: 0.0084 | Train Acc: 0.9999\n",
      "Test Loss: 1.3688 | Test Acc: 0.6359\n",
      "Epoch 53 took 4.57 seconds\n",
      "Epoch [54/100] | Train Loss: 0.0075 | Train Acc: 0.9999\n",
      "Test Loss: 1.3519 | Test Acc: 0.6419\n",
      "Epoch 54 took 4.52 seconds\n",
      "Epoch [55/100] | Train Loss: 0.0071 | Train Acc: 1.0000\n",
      "Test Loss: 1.3911 | Test Acc: 0.6379\n",
      "Epoch 55 took 4.52 seconds\n",
      "Epoch [56/100] | Train Loss: 0.0072 | Train Acc: 0.9999\n",
      "Test Loss: 1.4163 | Test Acc: 0.6321\n",
      "Epoch 56 took 4.53 seconds\n",
      "Epoch [57/100] | Train Loss: 0.0070 | Train Acc: 1.0000\n",
      "Test Loss: 1.3787 | Test Acc: 0.6364\n",
      "Epoch 57 took 4.51 seconds\n",
      "Epoch [58/100] | Train Loss: 0.0068 | Train Acc: 1.0000\n",
      "Test Loss: 1.4012 | Test Acc: 0.6365\n",
      "Epoch 58 took 4.52 seconds\n",
      "Epoch [59/100] | Train Loss: 0.0063 | Train Acc: 1.0000\n",
      "Test Loss: 1.4504 | Test Acc: 0.6327\n",
      "Epoch 59 took 4.54 seconds\n",
      "Epoch [60/100] | Train Loss: 0.0061 | Train Acc: 1.0000\n",
      "Test Loss: 1.3469 | Test Acc: 0.6414\n",
      "Epoch 60 took 4.54 seconds\n",
      "Epoch [61/100] | Train Loss: 0.0060 | Train Acc: 1.0000\n",
      "Test Loss: 1.3740 | Test Acc: 0.6405\n",
      "Epoch 61 took 4.51 seconds\n",
      "Epoch [62/100] | Train Loss: 0.0060 | Train Acc: 1.0000\n",
      "Test Loss: 1.5269 | Test Acc: 0.6187\n",
      "Epoch 62 took 4.50 seconds\n",
      "Epoch [63/100] | Train Loss: 0.0056 | Train Acc: 1.0000\n",
      "Test Loss: 1.4427 | Test Acc: 0.6313\n",
      "Epoch 63 took 4.52 seconds\n",
      "Epoch [64/100] | Train Loss: 0.0057 | Train Acc: 1.0000\n",
      "Test Loss: 1.3872 | Test Acc: 0.6382\n",
      "Epoch 64 took 4.55 seconds\n",
      "Epoch [65/100] | Train Loss: 0.0054 | Train Acc: 1.0000\n",
      "Test Loss: 1.4041 | Test Acc: 0.6342\n",
      "Epoch 65 took 4.51 seconds\n",
      "Epoch [66/100] | Train Loss: 0.0051 | Train Acc: 1.0000\n",
      "Test Loss: 1.4454 | Test Acc: 0.6356\n",
      "Epoch 66 took 4.54 seconds\n",
      "Epoch [67/100] | Train Loss: 0.0052 | Train Acc: 1.0000\n",
      "Test Loss: 1.4132 | Test Acc: 0.6320\n",
      "Epoch 67 took 4.51 seconds\n",
      "Epoch [68/100] | Train Loss: 0.0051 | Train Acc: 1.0000\n",
      "Test Loss: 1.4174 | Test Acc: 0.6374\n",
      "Epoch 68 took 4.55 seconds\n",
      "Epoch [69/100] | Train Loss: 0.0052 | Train Acc: 1.0000\n",
      "Test Loss: 1.4253 | Test Acc: 0.6356\n",
      "Epoch 69 took 4.51 seconds\n",
      "Epoch [70/100] | Train Loss: 0.0046 | Train Acc: 1.0000\n",
      "Test Loss: 1.4049 | Test Acc: 0.6374\n",
      "Epoch 70 took 4.51 seconds\n",
      "Epoch [71/100] | Train Loss: 0.0049 | Train Acc: 1.0000\n",
      "Test Loss: 1.4422 | Test Acc: 0.6351\n",
      "Epoch 71 took 4.53 seconds\n",
      "Epoch [72/100] | Train Loss: 0.0044 | Train Acc: 1.0000\n",
      "Test Loss: 1.4570 | Test Acc: 0.6330\n",
      "Epoch 72 took 4.54 seconds\n",
      "Epoch [73/100] | Train Loss: 0.0043 | Train Acc: 1.0000\n",
      "Test Loss: 1.4711 | Test Acc: 0.6312\n",
      "Epoch 73 took 4.51 seconds\n",
      "Epoch [74/100] | Train Loss: 0.0042 | Train Acc: 1.0000\n",
      "Test Loss: 1.6520 | Test Acc: 0.6155\n",
      "Epoch 74 took 4.52 seconds\n",
      "Epoch [75/100] | Train Loss: 0.0042 | Train Acc: 1.0000\n",
      "Test Loss: 1.4449 | Test Acc: 0.6336\n",
      "Epoch 75 took 4.50 seconds\n",
      "Epoch [76/100] | Train Loss: 0.0042 | Train Acc: 1.0000\n",
      "Test Loss: 1.3917 | Test Acc: 0.6378\n",
      "Epoch 76 took 4.51 seconds\n",
      "Epoch [77/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4110 | Test Acc: 0.6353\n",
      "Epoch 77 took 4.54 seconds\n",
      "Epoch [78/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4314 | Test Acc: 0.6362\n",
      "Epoch 78 took 4.51 seconds\n",
      "Epoch [79/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4328 | Test Acc: 0.6376\n",
      "Epoch 79 took 4.54 seconds\n",
      "Epoch [80/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.4429 | Test Acc: 0.6363\n",
      "Epoch 80 took 4.51 seconds\n",
      "Epoch [81/100] | Train Loss: 0.0038 | Train Acc: 1.0000\n",
      "Test Loss: 1.4130 | Test Acc: 0.6380\n",
      "Epoch 81 took 4.54 seconds\n",
      "Epoch [82/100] | Train Loss: 0.0040 | Train Acc: 1.0000\n",
      "Test Loss: 1.3891 | Test Acc: 0.6383\n",
      "Epoch 82 took 4.52 seconds\n",
      "Epoch [83/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4276 | Test Acc: 0.6365\n",
      "Epoch 83 took 4.53 seconds\n",
      "Epoch [84/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4581 | Test Acc: 0.6370\n",
      "Epoch 84 took 4.52 seconds\n",
      "Epoch [85/100] | Train Loss: 0.0040 | Train Acc: 0.9999\n",
      "Test Loss: 1.4222 | Test Acc: 0.6394\n",
      "Epoch 85 took 4.49 seconds\n",
      "Epoch [86/100] | Train Loss: 0.0039 | Train Acc: 1.0000\n",
      "Test Loss: 1.4825 | Test Acc: 0.6312\n",
      "Epoch 86 took 4.54 seconds\n",
      "Epoch [87/100] | Train Loss: 0.0036 | Train Acc: 1.0000\n",
      "Test Loss: 1.5096 | Test Acc: 0.6299\n",
      "Epoch 87 took 4.54 seconds\n",
      "Epoch [88/100] | Train Loss: 0.0036 | Train Acc: 1.0000\n",
      "Test Loss: 1.5379 | Test Acc: 0.6256\n",
      "Epoch 88 took 4.49 seconds\n",
      "Epoch [89/100] | Train Loss: 0.0034 | Train Acc: 1.0000\n",
      "Test Loss: 1.4169 | Test Acc: 0.6363\n",
      "Epoch 89 took 4.51 seconds\n",
      "Epoch [90/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.4327 | Test Acc: 0.6358\n",
      "Epoch 90 took 4.54 seconds\n",
      "Epoch [91/100] | Train Loss: 0.0034 | Train Acc: 1.0000\n",
      "Test Loss: 1.4911 | Test Acc: 0.6290\n",
      "Epoch 91 took 4.50 seconds\n",
      "Epoch [92/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.4422 | Test Acc: 0.6340\n",
      "Epoch 92 took 4.50 seconds\n",
      "Epoch [93/100] | Train Loss: 0.0032 | Train Acc: 1.0000\n",
      "Test Loss: 1.4214 | Test Acc: 0.6404\n",
      "Epoch 93 took 4.50 seconds\n",
      "Epoch [94/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.4077 | Test Acc: 0.6402\n",
      "Epoch 94 took 4.52 seconds\n",
      "Epoch [95/100] | Train Loss: 0.0033 | Train Acc: 1.0000\n",
      "Test Loss: 1.4485 | Test Acc: 0.6372\n",
      "Epoch 95 took 4.52 seconds\n",
      "Epoch [96/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.4574 | Test Acc: 0.6347\n",
      "Epoch 96 took 4.54 seconds\n",
      "Epoch [97/100] | Train Loss: 0.0035 | Train Acc: 1.0000\n",
      "Test Loss: 1.3983 | Test Acc: 0.6434\n",
      "Epoch 97 took 4.52 seconds\n",
      "Epoch [98/100] | Train Loss: 0.0032 | Train Acc: 1.0000\n",
      "Test Loss: 1.4488 | Test Acc: 0.6362\n",
      "Epoch 98 took 4.51 seconds\n",
      "Epoch [99/100] | Train Loss: 0.0033 | Train Acc: 1.0000\n",
      "Test Loss: 1.4781 | Test Acc: 0.6326\n",
      "Epoch 99 took 4.53 seconds\n",
      "Epoch [100/100] | Train Loss: 0.0033 | Train Acc: 1.0000\n",
      "Test Loss: 1.5807 | Test Acc: 0.6220\n",
      "Epoch 100 took 4.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:05:22,631 - INFO - Train accuracy 1.0, Train Loss 0.004078457265027932\n",
      "2025-02-12 16:05:22,632 - INFO - Test accuracy 0.62208, Test Loss 1.5799390868264802\n",
      "2025-02-12 16:05:22,650 - INFO - Training model 2 took 454.7932333946228 seconds\n"
     ]
    }
   ],
   "source": [
    "data_splits = [target_data_split, ref_data_split, ref_paired_data_split]\n",
    "memberships = np.array([target_membership, ref_membership, ref_paired_membership]) # size: 2N+1 * len(dataset)\n",
    "models_list = train_models(\n",
    "    log_dir, dataset, data_splits, memberships, configs, logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:05:22,775 - INFO - Loading model 0\n",
      "/home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "2025-02-12 16:05:22,836 - INFO - Loading model 1\n",
      "2025-02-12 16:05:22,877 - INFO - Loading model 2\n"
     ]
    }
   ],
   "source": [
    "models_list, memberships = load_models(log_dir, dataset, 3, configs, logger)\n",
    "target_membership, ref_membership, ref_paired_membership = memberships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the proportion $p$ of the dataset used\n",
    "We have imported the DUCI module using: `from modules.duci import DUCI`. In DUCI, the standard MIA is executed first, so we need to set up the MIA before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the population dataset used in MIA\n",
    "population = Subset(\n",
    "    population,\n",
    "    np.random.choice(\n",
    "        len(population),\n",
    "        configs[\"audit\"].get(\"population_size\", len(population)),\n",
    "        replace=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the victim model and the reference model to generate signals (softmax outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:05:23,187 - INFO - Computing signals for all models.\n",
      "Computing softmax: 100%|██████████| 1/1 [00:00<00:00, 20.68it/s]\n",
      "Computing softmax: 100%|██████████| 1/1 [00:00<00:00, 52.46it/s]\n",
      "Computing softmax: 100%|██████████| 1/1 [00:00<00:00, 52.20it/s]\n",
      "2025-02-12 16:05:23,324 - INFO - Signals saved to disk.\n",
      "2025-02-12 16:05:24,387 - INFO - Computing signals for all models.\n",
      "Computing softmax: 100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
      "Computing softmax: 100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
      "Computing softmax: 100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
      "2025-02-12 16:05:25,529 - INFO - Signals saved to disk.\n",
      "2025-02-12 16:05:25,542 - INFO - Preparing signals took 2.60461 seconds\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "auditing_dataset = Subset(dataset, target_indices)\n",
    "auditing_membership = np.array([target_membership[target_indices], ref_membership[target_indices], ref_paired_membership[target_indices]]).astype(bool) # size: 2 * len(auditing_dataset)   \n",
    "signals = get_model_signals(models_list, auditing_dataset, configs, logger) # num_samples * num_models\n",
    "auditing_membership = auditing_membership.T\n",
    "assert signals.shape == auditing_membership.shape, f\"signals or auditing_membership has incorrect shape (num_samples * num_models): {signals.shape} vs {auditing_membership.shape}\"\n",
    "population_signals = get_model_signals(\n",
    "    models_list, population, configs, logger, is_population=True\n",
    ")\n",
    "logger.info(\"Preparing signals took %0.5f seconds\", time.time() - baseline_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 0.5, 0.5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auditing_membership[:, 0].mean(), auditing_membership[:, 1].mean(), auditing_membership[:, 2].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform DUCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:05:25,573 - INFO - Initiate DUCI for target models: 0\n",
      "2025-02-12 16:05:25,575 - INFO - Collecting membership prediction for each sample in the target dataset on target models and reference models.\n",
      "2025-02-12 16:05:25,576 - INFO - Predicting the proportion of dataset usage on target models.\n",
      "2025-02-12 16:05:25,577 - INFO - Args for MIA attack: {'attack': 'RMIA', 'dataset': 'cifar10', 'model': 'wrn28-2', 'offline_a': 0.2}\n",
      "2025-02-12 16:05:25,579 - INFO - Running RMIA attack on target model 0 with offline_a=0.2\n",
      "2025-02-12 16:05:25,599 - INFO - Collect membership prediction for target dataset on target model 0 costs 0.0 seconds\n",
      "2025-02-12 16:05:25,600 - INFO - Args for MIA attack: {'attack': 'RMIA', 'dataset': 'cifar10', 'model': 'wrn28-2', 'offline_a': 0.2}\n",
      "2025-02-12 16:05:25,601 - INFO - Running RMIA attack on target model 1 with offline_a=0.2\n",
      "2025-02-12 16:05:25,614 - INFO - Args for MIA attack: {'attack': 'RMIA', 'dataset': 'cifar10', 'model': 'wrn28-2', 'offline_a': 0.2}\n",
      "2025-02-12 16:05:25,616 - INFO - Running RMIA attack on target model 2 with offline_a=0.2\n",
      "2025-02-12 16:05:25,634 - INFO - Best threshold = 0.5491 (Maximize TPR - FPR) = 0.972 - 0.352\n",
      "2025-02-12 16:05:25,636 - INFO - True proportion 0.2, True TPR: 0.97, True FPR: 0.385\n",
      "2025-02-12 16:05:25,637 - INFO - DUCI prediction: 0.2419354838709677; Direct Aggregation: 0.502\n",
      "2025-02-12 16:05:25,637 - INFO - Absolute Error $| \\hat{p} - p|$: Debiased Agg MIA = 0.0419\n",
      "2025-02-12 16:05:25,638 - INFO - DUCI 0.1 seconds\n",
      "2025-02-12 16:05:25,639 - INFO - Average prediction errors: 0.04193548387096768\n",
      "2025-02-12 16:05:25,640 - INFO - All prediction errors: [0.04193548387096768]\n",
      "2025-02-12 16:05:25,642 - INFO - Prediction details: DUCI predictions: [0.2419354838709677], True proportions: [0.2]\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "target_model_idx = 0\n",
    "ref_model_indices = [1, 2]\n",
    "\n",
    "logger.info(f\"Initiate DUCI for target models: {target_model_idx}\")\n",
    "\n",
    "args = {\n",
    "    \"attack\": \"RMIA\",\n",
    "    \"dataset\": configs[\"data\"][\"dataset\"], # TODO: have DUCI config\n",
    "    \"model\": configs[\"train\"][\"model_name\"],\n",
    "    \"offline_a\": 0.2 # If set to None, an extra reference model is required to tune the offline_a\n",
    "}\n",
    "# Initialize MIA instance\n",
    "MIA_instance = MIA(logger)\n",
    "DUCI_instance = DUCI(MIA_instance, logger, args)\n",
    "\n",
    "logger.info(\"Collecting membership prediction for each sample in the target dataset on target models and reference models.\")\n",
    "logger.info(\"Predicting the proportion of dataset usage on target models.\")\n",
    "\n",
    "duci_preds, true_proportions, errors = DUCI_instance.pred_proportions(\n",
    "    [target_model_idx], \n",
    "    [ref_model_indices], \n",
    "    signals,\n",
    "    population_signals,\n",
    "    auditing_membership,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"DUCI %0.1f seconds\", time.time() - baseline_time\n",
    ")\n",
    "logger.info(f\"Average prediction errors: {np.mean(errors)}\")\n",
    "logger.info(f\"All prediction errors: {errors}\")\n",
    "logger.info(f\"Prediction details: DUCI predictions: {duci_preds}, True proportions: {true_proportions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the prediction and $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground-truth proportion $p$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2419354838709677"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duci_preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the Proportion $p$ of a Dataset Used with a *Single* Reference Model  \n",
    "\n",
    "In our implementation using **Privacy Meter**, we follow the commonly used **half-half data split setting** from the MIA literature. Under this setting, using $N$ reference models for an offline attack means each target sample has $N$ reference models trained without it. However, due to the half-half split, a total of $2N$ models are trained. This implies that the **minimal number of trained models is 2**.  \n",
    "\n",
    "In **dataset usage inference**, we aim to infer dataset usage for the **entire dataset** (with $|X|$ samples) while using only a **single reference model**, rather than training separate offline models for all data points. The key question is: **can we achieve dataset usage inference without ensuring every data point has a corresponding offline model?**  \n",
    "\n",
    "### Adaptations to Achieve Single Reference Model Inference  \n",
    "\n",
    "1. **Denominator Approximation Using In-Model Estimates**  \n",
    "   - In offline **RMIA**, the denominator is approximated as $aP(x|\\theta_{\\text{out}}) + a -1 $ using out-models.  \n",
    "   - We extend this idea by approximating the denominator **using in-model estimates**: $(P(x|\\theta_{\\text{in}}) + a - 1)/a$\n",
    "   - With a half-half split, for samples included in the **single reference model** training, we estimate probabilities using the **in-world** model. For the remaining samples, we follow **RMIA** and approximate the denominator using **out-world** models.\n",
    "\n",
    "2. **Eliminating the Denominator to Transition to a Population Attack**  \n",
    "   - With a **single reference model**, the denominators in the RMIA scores (i.e., $P(x)$ and $P(z)$) **no longer approximate probabilities**, but instead act as a **linear transformation** of the numerator.  \n",
    "   - Given this, we **drop the denominator** for both $x$ and $z$, transforming the method into a **population attack**.\n",
    "\n",
    "This adaptation allows us to infer dataset usage **without requiring an extensive set of reference models**, while still leveraging the insights from RMIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MyMIA class to implement the updated version of RMIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "class MyMIA(MIA):\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        super().__init__(logger)\n",
    "    \n",
    "    def run_mia(\n",
    "        self,\n",
    "        all_signals: np.ndarray,\n",
    "        all_memberships: np.ndarray,\n",
    "        target_model_idx: int,\n",
    "        reference_model_indices: np.ndarray,\n",
    "        logger: logging.Logger,\n",
    "        args: Dict[str, Any],\n",
    "        population_signals: Optional[np.ndarray] = None,\n",
    "        reuse_offline_a: Optional[bool] = False,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Custom implementation of the MIA attack.\n",
    "        \"\"\"\n",
    "        assert all_signals.shape == all_memberships.shape, (\n",
    "            f\"all_signals and all_memberships must have the same shape: {all_signals.shape} vs {all_memberships.shape}\"\n",
    "        )\n",
    "        \n",
    "        target_signals = all_signals[:, target_model_idx]\n",
    "        target_memberships = all_memberships[:, target_model_idx]\n",
    "\n",
    "        ref_signals = all_signals[:, reference_model_indices]\n",
    "        ref_memberships = all_memberships[:, reference_model_indices]\n",
    "\n",
    "        z_target_signals = population_signals[:, target_model_idx]\n",
    "        z_ref_signals = population_signals[:, reference_model_indices]\n",
    "\n",
    "        logger.info(f\"Args for MyMIA attack: {args}\")\n",
    "        \n",
    "        assert population_signals is not None, \"population_signals is required for RMIA attack\"\n",
    "        assert args.get(\"offline_a\") is not None, \"offline_a is required for single model RMIA attack\"\n",
    "            \n",
    "        offline_a = args[\"offline_a\"]\n",
    "            \n",
    "        logger.info(f\"Running MyRMIA attack on target model {target_model_idx} with offline_a={offline_a}\")\n",
    "        mia_scores = self.my_rmia(target_signals, z_target_signals, offline_a)\n",
    "        \n",
    "        return mia_scores, target_memberships\n",
    "    \n",
    "    def my_rmia(\n",
    "        self,\n",
    "        target_signals: np.ndarray,\n",
    "        z_target_signals: np.ndarray,\n",
    "        offline_a: float,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Attack a target model using the RMIA attack with the help of offline reference models.\n",
    "\n",
    "        Args:\n",
    "            target_signals (np.ndarray): Softmax value of all samples in the target model.\n",
    "            ref_signals (np.ndarray): Softmax value of all samples in the reference models.\n",
    "            ref_memberships (np.ndarray): Membership matrix for all reference models.\n",
    "            z_target_signals (np.ndarray): Softmax value of population samples in the target model.\n",
    "            z_ref_signals (np.ndarray): Softmax value of population samples in all reference models.\n",
    "            offline_a (float): Coefficient offline_a is used to approximate p(x) using P_out in the offline setting.\n",
    "            num_reference_models (Optional[int]): Number of reference models used for the attack. Defaults to half reference models if None.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: MIA score for all samples (a larger score indicates higher chance of being member).\n",
    "        \"\"\"\n",
    "        prob_ratio_x = target_signals.ravel()\n",
    "        prob_ratio_z = z_target_signals.ravel()\n",
    "\n",
    "        ratios = prob_ratio_x[:, np.newaxis] / prob_ratio_z\n",
    "        counts = np.average(ratios > 1.0, axis=1)\n",
    "\n",
    "        return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:05:25,706 - INFO - Initiate DUCI for target models: 0\n",
      "2025-02-12 16:05:25,708 - INFO - Collecting membership prediction for each sample in the target dataset on target models and reference models.\n",
      "2025-02-12 16:05:25,709 - INFO - Predicting the proportion of dataset usage on target models.\n",
      "2025-02-12 16:05:25,711 - INFO - Args for MyMIA attack: {'attack': 'RMIA', 'dataset': 'cifar10', 'model': 'wrn28-2', 'offline_a': 0.3}\n",
      "2025-02-12 16:05:25,712 - INFO - Running MyRMIA attack on target model 0 with offline_a=0.3\n",
      "2025-02-12 16:05:25,727 - INFO - Collect membership prediction for target dataset on target model 0 costs 0.0 seconds\n",
      "2025-02-12 16:05:25,728 - INFO - Args for MyMIA attack: {'attack': 'RMIA', 'dataset': 'cifar10', 'model': 'wrn28-2', 'offline_a': 0.3}\n",
      "2025-02-12 16:05:25,729 - INFO - Running MyRMIA attack on target model 1 with offline_a=0.3\n",
      "2025-02-12 16:05:25,742 - INFO - Best threshold = 0.6838 (Maximize TPR - FPR) = 0.98 - 0.364\n",
      "2025-02-12 16:05:25,743 - INFO - True proportion 0.2, True TPR: 0.99, True FPR: 0.315\n",
      "2025-02-12 16:05:25,745 - INFO - DUCI prediction: 0.13961038961038966; Direct Aggregation: 0.45\n",
      "2025-02-12 16:05:25,746 - INFO - Absolute Error $| \\hat{p} - p|$: Debiased Agg MIA = 0.0604\n",
      "2025-02-12 16:05:25,747 - INFO - DUCI 0.0 seconds\n",
      "2025-02-12 16:05:25,748 - INFO - Average prediction errors: 0.060389610389610354\n",
      "2025-02-12 16:05:25,749 - INFO - All prediction errors: [0.060389610389610354]\n",
      "2025-02-12 16:05:25,750 - INFO - Prediction details: DUCI predictions: [0.13961038961038966], True proportions: [0.2]\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "target_model_idx = 0\n",
    "ref_model_indices = [1]\n",
    "\n",
    "logger.info(f\"Initiate DUCI for target models: {target_model_idx}\")\n",
    "\n",
    "args = {\n",
    "    \"attack\": \"RMIA\",\n",
    "    \"dataset\": configs[\"data\"][\"dataset\"], # TODO: have DUCI config\n",
    "    \"model\": configs[\"train\"][\"model_name\"],\n",
    "    \"offline_a\": 0.3 # If set to None, an extra reference model is required to tune the offline_a\n",
    "}\n",
    "# Initialize MIA instance\n",
    "MIA_instance = MyMIA(logger)\n",
    "DUCI_instance = DUCI(MIA_instance, logger, args)\n",
    "\n",
    "logger.info(\"Collecting membership prediction for each sample in the target dataset on target models and reference models.\")\n",
    "logger.info(\"Predicting the proportion of dataset usage on target models.\")\n",
    "\n",
    "duci_preds, true_proportions, errors = DUCI_instance.pred_proportions(\n",
    "    [target_model_idx], \n",
    "    [ref_model_indices], \n",
    "    signals,\n",
    "    population_signals,\n",
    "    auditing_membership,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"DUCI %0.1f seconds\", time.time() - baseline_time\n",
    ")\n",
    "logger.info(f\"Average prediction errors: {np.mean(errors)}\")\n",
    "logger.info(f\"All prediction errors: {errors}\")\n",
    "logger.info(f\"Prediction details: DUCI predictions: {duci_preds}, True proportions: {true_proportions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the prediction and $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground-truth proportions: 0.2 | Our DUCI predictions: 0.13961038961038966\n"
     ]
    }
   ],
   "source": [
    "print(f\"The ground-truth proportions: {p} | Our DUCI predictions: {duci_preds[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing the most naive loss attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "class MyMIA(MIA):\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        super().__init__(logger)\n",
    "    \n",
    "    def run_mia(\n",
    "        self,\n",
    "        all_signals: np.ndarray,\n",
    "        all_memberships: np.ndarray,\n",
    "        target_model_idx: int,\n",
    "        reference_model_indices: np.ndarray,\n",
    "        logger: logging.Logger,\n",
    "        args: Dict[str, Any],\n",
    "        population_signals: Optional[np.ndarray] = None,\n",
    "        reuse_offline_a: Optional[bool] = False,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Custom implementation of the MIA attack.\n",
    "        \"\"\"\n",
    "        assert all_signals.shape == all_memberships.shape, (\n",
    "            f\"all_signals and all_memberships must have the same shape: {all_signals.shape} vs {all_memberships.shape}\"\n",
    "        )\n",
    "        \n",
    "        target_signals = all_signals[:, target_model_idx]\n",
    "        target_memberships = all_memberships[:, target_model_idx]\n",
    "\n",
    "        ref_signals = all_signals[:, reference_model_indices]\n",
    "        ref_memberships = all_memberships[:, reference_model_indices]\n",
    "\n",
    "        z_target_signals = population_signals[:, target_model_idx]\n",
    "        z_ref_signals = population_signals[:, reference_model_indices]\n",
    "\n",
    "        logger.info(f\"Args for MyMIA attack: {args}\")\n",
    "        \n",
    "        assert population_signals is not None, \"population_signals is required for RMIA attack\"\n",
    "        assert args.get(\"offline_a\") is not None, \"offline_a is required for single model RMIA attack\"\n",
    "            \n",
    "        offline_a = args[\"offline_a\"]\n",
    "            \n",
    "        logger.info(f\"Running MyRMIA attack on target model {target_model_idx} with offline_a={offline_a}\")\n",
    "        mia_scores = self.my_rmia(target_signals, z_target_signals, offline_a)\n",
    "        \n",
    "        return mia_scores, target_memberships\n",
    "    \n",
    "    def my_rmia(\n",
    "        self,\n",
    "        target_signals: np.ndarray,\n",
    "        ref_signals: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Attack a target model using the RMIA attack with the help of offline reference models.\n",
    "\n",
    "        Args:\n",
    "            target_signals (np.ndarray): Softmax value of all samples in the target model.\n",
    "            ref_signals (np.ndarray): Softmax value of all samples in the reference models.\n",
    "            ref_memberships (np.ndarray): Membership matrix for all reference models.\n",
    "            z_target_signals (np.ndarray): Softmax value of population samples in the target model.\n",
    "            z_ref_signals (np.ndarray): Softmax value of population samples in all reference models.\n",
    "            offline_a (float): Coefficient offline_a is used to approximate p(x) using P_out in the offline setting.\n",
    "            num_reference_models (Optional[int]): Number of reference models used for the attack. Defaults to half reference models if None.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: MIA score for all samples (a larger score indicates higher chance of being member).\n",
    "        \"\"\"\n",
    "\n",
    "        return target_signals.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_time = time.time()\n",
    "target_model_idx = 0\n",
    "ref_model_indices = [1]\n",
    "\n",
    "logger.info(f\"Initiate DUCI for target models: {target_model_idx}\")\n",
    "\n",
    "args = {\n",
    "    \"attack\": \"RMIA\",\n",
    "    \"dataset\": configs[\"data\"][\"dataset\"], # TODO: have DUCI config\n",
    "    \"model\": configs[\"train\"][\"model_name\"],\n",
    "    \"offline_a\": 0.3 # If set to None, an extra reference model is required to tune the offline_a\n",
    "}\n",
    "# Initialize MIA instance\n",
    "MIA_instance = MyMIA(logger)\n",
    "DUCI_instance = DUCI(MIA_instance, logger, args)\n",
    "\n",
    "logger.info(\"Collecting membership prediction for each sample in the target dataset on target models and reference models.\")\n",
    "logger.info(\"Predicting the proportion of dataset usage on target models.\")\n",
    "\n",
    "duci_preds, true_proportions, errors = DUCI_instance.pred_proportions(\n",
    "    [target_model_idx], \n",
    "    [ref_model_indices], \n",
    "    signals,\n",
    "    population_signals,\n",
    "    auditing_membership,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"DUCI %0.1f seconds\", time.time() - baseline_time\n",
    ")\n",
    "logger.info(f\"Average prediction errors: {np.mean(errors)}\")\n",
    "logger.info(f\"All prediction errors: {errors}\")\n",
    "logger.info(f\"Prediction details: DUCI predictions: {duci_preds}, True proportions: {true_proportions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
